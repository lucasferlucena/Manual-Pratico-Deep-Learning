{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Introdução](#Introdução)\n",
    "\n",
    "[Função de Custo](#Função-de-Custo)\n",
    "\n",
    "[Regressão Logística](#Regressão-Log%C3%ADstica)\n",
    "\n",
    "[Exercícios](#Exercícios)\n",
    "\n",
    "[Referências](#Referências)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A __Regressão Logística__, apesar do nome, é uma técnica utilizada para fazer __classificação binária__. Nesse caso, ao invés de prever um valor contínuo, a nossa __saída é composta de apenas dois valores: 0 ou 1__, em geral. Para fazer a regressão logística, utilizamos como função de ativação a função conhecida como __sigmoid__. Tal função, é descrita pela seguinte fórmula:\n",
    "\n",
    "$$\\widehat{y} = \\frac{1}{1+e^{-z}} = \\frac{e^z}{1+e^z}$$\n",
    "\n",
    "No caso de redes neurais, em geral consideramos $z(w,b) = xw^T + b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função de Custo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função de custo da regressão logística é chamada de __entropia cruzada__ (do inglês, __cross-entropy__) e é definida pela seguinte fórmula:\n",
    "\n",
    "$$J(z) = -\\frac{1}{N}\\sum_{i}^N y_i\\log(\\widehat{y}_i) + (1-y_i)\\log(1-\\widehat{y}_i)$$\n",
    "\n",
    "Onde $N$ é quantidade de amostras e $y_i$ representa o valor da $i$-ésima amostra (0 ou 1). Lembrando que $\\widehat{y}_i$ é agora calculada agora utilizando a função ___sigmoid___, como mostrado na seção anterior.\n",
    "\n",
    "Repare também que:\n",
    "\n",
    "- quando $y_i = 0$, o primeiro termo anula-se (pois $y_i = 0$). Logo, vamos considerar os dois casos extremos para $\\widehat{y}_i = 0$ no segundo termo da equação ($(1-y_i)\\log(1-\\widehat{y}_i)$):\n",
    "    - quando $\\widehat{y}_i = 0$, temos que o $\\log(1-\\widehat{y}_i) = \\log(1) = 0$. Logo, o nosso custo $J = 0$. Repare que isso faz todo sentido, pois $y_i = 0$ e $\\widehat{y}_i = 0$. \n",
    "    - quando $\\widehat{y}_i = 1$, temos que o $\\log(1-\\widehat{y}_i) = \\log(0) = \\infty$. Agora, o nosso custo $J = \\infty$. Ou seja, quanto mais diferente são $y_i$ e $\\widehat{y}_i$, maior o nosso custo.\n",
    "- quando $y_i = 1$, o segundo termo anula-se (pois $(1-y_i) = 0$). Novamente, vamos considerar os dois casos extremos para $\\widehat{y}_i = 0$, só que agora no primeiro termo da equação ($y_i\\log(\\widehat{y}_i)$):\n",
    "    - quando $\\widehat{y}_i = 0$, temos que o $\\log(\\widehat{y}_i) = \\infty$. Logo, o nosso custo $J = \\infty$. Novamente, como $y_i$ e $\\widehat{y}_i$ são bem diferentes, o custo tende a aumentar.\n",
    "    - quando $\\widehat{y}_i = 1$, temos que $\\log(\\widehat{y}_i) = \\log(1) = 0$. Agora, o nosso custo $J = 0$. Novamente, isso faz todo sentido, pois $y_i = 1$ e $\\widehat{y}_i = 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivada da Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular a derivada da nossa função de custo $J(z)$, primeiramente vamos calcular $\\log(\\widehat{y}_i)$:\n",
    "\n",
    "$$\\log(\\widehat{y}_i) = log\\frac{1}{1+e^{-z}} = log(1) - log(1+e^{-z}) = -log(1+e^{-z})$$\n",
    "\n",
    "E $\\log(1-\\widehat{y}_i)$:\n",
    "\n",
    "$$\\log(1-\\widehat{y}_i) = log \\left(1-\\frac{1}{1+e^{-z}}\\right) = log(e^{-z}) - log(1+e^{-z}) = -z -log(1+e^{-z})$$\n",
    "\n",
    "Substituindo as duas equações anteriores na fórmula da função de custo, temos:\n",
    "\n",
    "$$J(z) = -\\frac{1}{N}\\sum_{i}^N \\left[-y_i\\log(1+e^{-z}) + (1-y_i)(-z -\\log(1+e^{-z}))\\right]$$\n",
    "\n",
    "Efetuando as distribuições, podemos simplificar a equação acima para:\n",
    "\n",
    "$$J(z) = -\\frac{1}{N}\\sum_{i}^N \\left[y_iz -z -\\log(1+e^{-z})\\right]$$\n",
    "\n",
    "Uma vez que:\n",
    "\n",
    "$$-z -\\log(1+e^{-z}) = -\\left[\\log e^{z} + log(1+e^{-z})\\right] = -log(1+e^z)$$\n",
    "\n",
    "Temos:\n",
    "\n",
    "$$J(z) = -\\frac{1}{N}\\sum_{i}^N \\left[y_iz -\\log(1+e^z)\\right]$$\n",
    "\n",
    "Como a derivada da diferença é igual a diferença das derivadas, podemos calcular cada derivada individualmente em relação a $w$:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_i}y_iz = y_ix_i,\\quad \\frac{\\partial}{\\partial w_i}\\log(1+e^z) = \\frac{x_ie^z}{1+e^z} = x_i \\widehat{y}_i$$\n",
    "\n",
    "e em relação à $b$:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial b}y_iz = y_i,\\quad \\frac{\\partial}{\\partial b}\\log(1+e^z) = \\frac{e^z}{1+e^z} = \\widehat{y}_i$$\n",
    "\n",
    "Assim, a derivada da nossa função de custo $J(z)$ é:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_i}J(z) = \\sum_i^N (y_i - \\widehat{y}_i)x_i$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial b}J(z) = \\sum_i^N (y_i - \\widehat{y}_i)$$\n",
    "\n",
    "Por fim, repare que o __gradiente de J ($\\nabla J$) é exatamente o mesmo que o gradiente da função de custo do [Perceptron Linear](Perceptron.ipynb#Como-o-Perceptron-Aprende?)__. Portanto, os pesos serão atualizados da mesma maneira. O que muda é a forma como calculamos $\\widehat{y}$ (agora usando a função _sigmoid_) e a função de custo $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/anuncios.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df.idade.values.reshape(-1,1), df.comprou.values.reshape(-1,1)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, c=y, cmap='bwr')\n",
    "plt.xlabel('idade')\n",
    "plt.ylabel('comprou?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler(feature_range=(-1,1))\n",
    "x = minmax.fit_transform(x.astype(np.float64))\n",
    "\n",
    "print(x.min(), x.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar o _sklearn_ como gabarito para nossa implementação. Entretanto, como a Regressão Logística do _sklearn_ faz uma __regularização L2__ automaticamente, temos de definir $C=10^{15}$ para \"anular\" a regularização. O parâmetro $C$ define a inversa da força da regularização (ver [documentação](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)). __Logo, quanto menor for o $C$, maior será a regularização e menores serão os valores dos pesos e bias.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_sk = LogisticRegression(C=1e15)\n",
    "clf_sk.fit(x, y.ravel())\n",
    "\n",
    "print(clf_sk.coef_, clf_sk.intercept_)\n",
    "print(clf_sk.score(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(x.min(), x.max(), 100).reshape(-1,1)\n",
    "y_sk = clf_sk.predict_proba(x_test)\n",
    "\n",
    "plt.scatter(x, y, c=y, cmap='bwr')\n",
    "plt.plot(x_test, y_sk[:,1], color='black')\n",
    "plt.xlabel('idade')\n",
    "plt.ylabel('comprou?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implemente a função sigmoid aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implemente o neurônio sigmoid aqui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(x.min(), x.max(), 100).reshape(-1,1)\n",
    "y_sk = clf_sk.predict_proba(x_test)\n",
    "y_pred = sigmoid(np.dot(x_test, w.T) + b)\n",
    "\n",
    "plt.scatter(x, y, c=y, cmap='bwr')\n",
    "plt.plot(x_test, y_sk[:,1], color='black', linewidth=7.0)\n",
    "plt.plot(x_test, y_pred, color='yellow')\n",
    "plt.xlabel('idade')\n",
    "plt.ylabel('comprou?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Acurácia pelo Scikit-learn: {:.2f}%'.format(clf_sk.score(x, y)*100))\n",
    "\n",
    "y_pred = np.round(sigmoid(np.dot(x, w.T) + b))\n",
    "print('Acurária pela nossa implementação: {:.2f}%'.format(accuracy_score(y, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercícios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df[['idade', 'salario']].values, df.comprou.values.reshape(-1,1)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter3D(x[:,0], x[:,1], y, c=y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler(feature_range=(-1,1))\n",
    "x = minmax.fit_transform(x.astype(np.float64))\n",
    "\n",
    "print(x.min(), x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_sk = LogisticRegression(C=1e15)\n",
    "clf_sk.fit(x, y.ravel())\n",
    "\n",
    "print(clf_sk.coef_, clf_sk.intercept_)\n",
    "print(clf_sk.score(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = 2*np.random.random((1, D))-1 # [1x2]\n",
    "b = 2*np.random.random()-1       # [1x1]\n",
    "\n",
    "learning_rate = 1.0 # <- tente estimar a learning rate\n",
    "\n",
    "for step in range(1): # <- tente estimar a #epochs\n",
    "    # calcule a saida do neuronio sigmoid\n",
    "    z = \n",
    "    y_pred =\n",
    "    \n",
    "    error = y - y_pred     # [400x1]\n",
    "    \n",
    "    w = w + learning_rate*np.dot(error.T, x)\n",
    "    b = b + learning_rate*error.sum()\n",
    "    \n",
    "    if step%100 == 0:\n",
    "        # implemente a entropia cruzada (1 linhas)\n",
    "        cost = \n",
    "        print('step {0}: {1}'.format(step, cost))\n",
    "\n",
    "print('w: ', w)\n",
    "print('b: ', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(x[:, 0].min(), x[:, 0].max())\n",
    "x2 = np.linspace(x[:, 1].min(), x[:, 1].max())\n",
    "x1_mesh, x2_mesh = np.meshgrid(x1, x2)\n",
    "x1_mesh = x1_mesh.reshape(-1, 1)\n",
    "x2_mesh = x2_mesh.reshape(-1, 1)\n",
    "\n",
    "x_mesh = np.hstack((x1_mesh, x2_mesh))\n",
    "y_pred = sigmoid(np.dot(x_mesh, w.T) + b)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter3D(x[:,0], x[:,1], y, c=y.ravel())\n",
    "ax.plot_trisurf(x1_mesh.ravel(), x2_mesh.ravel(), y_pred.ravel(), alpha=0.3, shade=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Acurácia pelo Scikit-learn: {:.2f}%'.format(clf_sk.score(x, y)*100))\n",
    "\n",
    "y_pred = np.round(sigmoid(np.dot(x, w.T) + b))\n",
    "print('Acurária pela nossa implementação: {:.2f}%'.format(accuracy_score(y, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Logistic Regression from Scratch in Python](https://beckernick.github.io/logistic-regression-from-scratch/)\n",
    "2. [Derivative of cost function for logistic Regression](https://math.stackexchange.com/questions/477207/derivative-of-cost-function-for-logistic-regression)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
